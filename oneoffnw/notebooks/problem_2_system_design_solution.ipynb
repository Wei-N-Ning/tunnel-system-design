{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a793d6",
   "metadata": {},
   "source": [
    "# 2. Turn the algorithm into a service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831aea6",
   "metadata": {},
   "source": [
    "## Service high level view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89719bd1",
   "metadata": {},
   "source": [
    "This service would consist of 3 components as shown in the diagram below.\n",
    "\n",
    "- a really simple frontend component\n",
    "- a compute component, a cluster of virtual machines running an improved, hypothetical version of the \"working demo\" from Part 1\n",
    "- a storage component powered by an object store (S3) and a NoSQL database (DynamoDB) with index-based search functionality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb71c8",
   "metadata": {},
   "source": [
    "![image.png](./api_design.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3d67d",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbd405",
   "metadata": {},
   "source": [
    "- assume the service is to be deployed to a public, aws-like cloud platform (rather than in-house data center)\n",
    "- assume the users interface the system in an anonymous fashion, meaning no login is required. However, please see the \"System hardening\" section at the end for further thoughts.\n",
    "- assume the networks are defined in the same format as in Part. 1, i.e. each network is a list of triples `[start_location, end_location, edge_flow]`\n",
    "- assume the network definitions are stored in `.json` files to be uploaded by the client users\n",
    "- assume the users do not need to see the actual networks (in a graphical interface or so) but only the evaluated max-flow value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51e520",
   "metadata": {},
   "source": [
    "### User interaction and request life cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46e022",
   "metadata": {},
   "source": [
    "The client users would interact with the system in the following ways:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35427c4",
   "metadata": {},
   "source": [
    "#### Option 1. HTTP-POST to upload a network definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889085d5",
   "metadata": {},
   "source": [
    "The user can send an http-post request with a network `.json` file and a network description in plain text.\n",
    "\n",
    "- if the network file is in a valid format and the description is not empty, upload the file and return http 201\n",
    "- if otherwise, return http 400 with error description\n",
    "- if a network file exists server-side with the same description, return http 403 with \"resource exists\" error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bceb94",
   "metadata": {},
   "source": [
    "#### Option 2. HTTP-GET to query the maximum flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af2800",
   "metadata": {},
   "source": [
    "The user can send an http-get request with (start, end) locations and two types of parameters:\n",
    "- type 1: with specific network description, the system then returns the max-flow accordingly\n",
    "- type 2: without specific network description, the system then returns all the known \"versions\" (all the proposals).\n",
    "\n",
    "For example, if 3 engineers have submitted their network json files that design a particular route connecting Mascot to North Sydney:\n",
    "\n",
    "```\n",
    "Network Design by A\n",
    "Network Design by B\n",
    "Network Design by C\n",
    "``` \n",
    "\n",
    "if I issue a query command to the API, asking for the \"max flow capacity between Mascot and North Sydney with Network Design by A\", the system would return me a single record: \n",
    "\n",
    "```\n",
    "Network Design by A, from <Mascot> to <North Sydney>, max flow: 4000/h\n",
    "```\n",
    "\n",
    "Whereas if I issue a query command to the API, asking for \"max flow capacity between Mascot and North Sydney\", the system would give me three records:\n",
    "\n",
    "```\n",
    "Network Design by C, from <Mascot> to <North Sydney>, max flow: 1000/h\n",
    "Network Design by B, from <Mascot> to <North Sydney>, max flow: 2000/h\n",
    "Network Design by A, from <Mascot> to <North Sydney>, max flow: 4000/h\n",
    "```\n",
    "\n",
    "(assume that the system has already computed all 3 max-flow values and persisted the results)\n",
    "\n",
    "However, there is one **exception** with the second type of parameter.\n",
    "\n",
    "If the system can not find any record designated by (start, end), it returns http 404 with an error description, because it does not know which network the pair of locations (start, end) refer to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f9794",
   "metadata": {},
   "source": [
    "## Frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f1727",
   "metadata": {},
   "source": [
    "- Could either be an embedded element on the caller's site (such as Transport NSW), or\n",
    "- Dynamically served by a lambda function, or\n",
    "- A dedicated site served on our VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade67126",
   "metadata": {},
   "source": [
    "It would ensure the user submits sufficient information and perform basic validations, for example:\n",
    "\n",
    "- ensure the network description contain valid alphanumeric characters \n",
    "- ensure the json file exists and is in the valid format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aeb472",
   "metadata": {},
   "source": [
    "### Handle Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1469c7",
   "metadata": {},
   "source": [
    "The frontend would deserialize the client json and send it with the request.\n",
    "\n",
    "The size of the payload would be acceptable as in the extreme cases, where a network json file contains 40-80k roads, the file size would only be a few Mbs (with space characters stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44dbb21",
   "metadata": {},
   "source": [
    "## Compute component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0949c",
   "metadata": {},
   "source": [
    "![image.png](./compute_component.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925c436",
   "metadata": {},
   "source": [
    "The requests from the frontend would firstly land on the API Gateway, where they are dispatched to the backend cluster managed by an auto-scaling group. \n",
    "\n",
    "The design intention is:\n",
    "\n",
    "- API Gateway acts as the warden facing the hostile outside world (DDOS attack etc.)\n",
    "- the cluster runs in DMZ (demilitarized zone), a separate internal subnet that operates in a safe environment\n",
    "- the auto-scaling mechanism would use CPU utilization to scale up or down (for example: if utilization > 50%, scale up) to ensure system remains responsive and not overly idle \n",
    "- the compute instances are provisioned from a machine image (the green box above) with the latest security patches. See the deployment section for more details.\n",
    "- both API Gateway and the cluster have their own monitoring services, typically provided by the platform (such as AWS CloudWatch) or via trusted third-parties such as Dynatrace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca6d3f",
   "metadata": {},
   "source": [
    "### API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85261d0",
   "metadata": {},
   "source": [
    "To specify the end points (for query and upload) and harden security.\n",
    "\n",
    "Here is a brief comparison between provider-managed gateway (such as AWS API Gateway) and dedicated gateway instances. The pros and cons are:\n",
    "\n",
    "- managed gateway simplifies maintenance but is likely more expensive; does not expose every detail for fine-tuning.\n",
    "- gateway instances are just normal virtual machine instances with intense security measure; can be inexpensive and allows for easy customization (such as streaming and jump-host); require dedicated maintenance efforts\n",
    "\n",
    "Here I assume we have chosen to use a provider-managed API Gateway service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f21878",
   "metadata": {},
   "source": [
    "### Handle Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c68c6",
   "metadata": {},
   "source": [
    "The max-flow service, which is an improved version from the working demo in Part 1 would take the network definition wrapped in the json payload along with the network description.\n",
    "\n",
    "It would then send a PUT request via the S3 api to upload the network file to a bucket. This creates a blob object named after the description. \n",
    "\n",
    "Note that the PUT request would fail if the blob already exists (even with a different content). **This restricition is configured via bucket policy**. See the System hardening section for details. In this case the backend will return http 403 \"resource exists\" error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f81eff",
   "metadata": {},
   "source": [
    "### Handle Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece5676",
   "metadata": {},
   "source": [
    "In the case of user querying the max-flow, the backend would perform a few operations in the following order:\n",
    "\n",
    "Firstly, \n",
    "\n",
    "- if the user specifies the (start, end) locations and the network description, the system combines these two pieces of information to form a key, and try to find an record in the DynamoDB.\n",
    "\n",
    "- if the user does not specify the network description, it would use the (start, end) location as the search index and scan for all the matching keys. (recall that in the above example, the system would find three records, with A, B, C's network design respectively)\n",
    "\n",
    "If either situation is successful, the backend crafts a response with a single record (case 1) or a list of records (case 2) and returns it.\n",
    "\n",
    "If not successful, meaning that no records exist for the query, it would proceed to the second operation:\n",
    "\n",
    "It firstly sends a GET request via the S3 api to download the network file (the blob object). If not found it would result in http 404 without further processing.\n",
    "\n",
    "Then it uses the user-specified (start, end) locations to compute the max-flow. \n",
    "\n",
    "Note that if user does not provide the network description at all, the system will consider this an user error (the system doesn't know which network the (start, end) is referring to). An http 400 is returned with an error description.\n",
    "\n",
    "The result of the computation, the max-flow, will be sent to DynamoDB (PUT) with a new key: (start, end, network-description). This record is available for further queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f0004",
   "metadata": {},
   "source": [
    "## The Storage Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360791a",
   "metadata": {},
   "source": [
    "![image](./storage_component.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd643d",
   "metadata": {},
   "source": [
    "The storage component has the following invariants:\n",
    "\n",
    "- it only accepts traffic from the backend subnet (an internal subnet)\n",
    "- both the S3 bucket and DynamoDB enables multi-AZs (availability zones) backup to prevent data loss.\n",
    "- the bucket disables delete-object and disallows overriding existing blobs\n",
    "- enable indexing in DynamoDB, use the (start, end) locations, a tuple, as the index key (such that the compute component can efficiently search for all the records associated with this pair of locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a344834",
   "metadata": {},
   "source": [
    "### Handle Upload and Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fb56c",
   "metadata": {},
   "source": [
    "As explained in the compute component section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8914f8",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ffddb",
   "metadata": {},
   "source": [
    "I would choose the following deployment strategies:\n",
    "\n",
    "- use an infra-as-code system to provision all the above components; recommending Hashicorp's Terraform\n",
    "- separate **test**, **staging**, **production** VPCs; allow devs to test deployment in the **test** VPC anytime; only deploy to **staging** after merging and successful unit testing; only deploy to **production** once all the functional tests have passed in staging (see the testing section below)\n",
    "- use rolling deployment, taking advantage of auto-scaling (see the details below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c98aa3",
   "metadata": {},
   "source": [
    "### Infra as Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f9ee3",
   "metadata": {},
   "source": [
    "Basically, in the service repository, a subdirectory named \"infrastructure\" contains\n",
    "all the provision logic required to spin up and teardown the corresponding cloud components.\n",
    "\n",
    "A team or person working in the service source code would also update the infrastructure source code and make sure both parts always stay in-sync.\n",
    "\n",
    "Two separate \"build\" processes exist for both the service source code and the infrastructure. Typically the service needs certain provisional changes done before the developers can deploy the artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff96c3",
   "metadata": {},
   "source": [
    "### Rolling Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19697c",
   "metadata": {},
   "source": [
    "#### Deploymeny Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43dddd",
   "metadata": {},
   "source": [
    "Each time, a successful merge would lead to the creation of a new machine image (AMI) which contains the host OS, the artifacts (the binary + dependencies) and all the security patches (It should pass all the security compliance checking).\n",
    "\n",
    "This machine image is the deployment target. It is labelled with the commit, the development ticket number and other descriptive information.\n",
    "\n",
    "In the staging environment, a small cluster (auto-scaling group) is created with this machine image so that we can carry on functional + manual testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df0f33",
   "metadata": {},
   "source": [
    "#### Rolling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc36c0",
   "metadata": {},
   "source": [
    "Once a deployment target is thoroughly tested in the staging environment, it is relabeled the \"chosen machine image\" (which is the green box in the compute component diagram). \n",
    "\n",
    "The production auto-scaling group sets this chosen machine image as its provision template.\n",
    "\n",
    "Then an automated process periodically increments the scaling factor to spawn more instances with the new machine image. \n",
    "\n",
    "The cluster's monitoring system would raise an alarm if any new instances throw errors. In such cases, the automated process would reset the machine image to the previous one (rollback) and kill off the newly spawned instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c0e52",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b2ea1",
   "metadata": {},
   "source": [
    "I would use two types of monitoring strategies for different resources.\n",
    "\n",
    "- security-oriented monitoring methods for critical resources, such as the API Gateway\n",
    "- performance-oriented methods for data intensive resources, such as the instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17fd4af",
   "metadata": {},
   "source": [
    "### Security-oriented monitoring objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b9f42",
   "metadata": {},
   "source": [
    "To keep an eye on:\n",
    "\n",
    "- DDOS attacks\n",
    "- port brutal-forcing (which can be less an issue for managed API Gateway)\n",
    "- malicious staff\n",
    "\n",
    "The corresponding data sources to keep track of are:\n",
    "\n",
    "- Gateway traffic dashboard, to look for highly repetitive requests from a small set of IP addresses\n",
    "- Gateway error graph (to look for high volumes of port rejection) \n",
    "- Alarms sent from the login trail (such as AWS cloudtrail) that indicate possible permission escalation\n",
    "\n",
    "All the security-oriented warnings should be treated as alarms and dealt with immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb45b45",
   "metadata": {},
   "source": [
    "### Performance-oriented monitoring objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310e3d0",
   "metadata": {},
   "source": [
    "I would apply the [USE](http://www.brendangregg.com/usemethod.html) method, which recommends keeping track of the following metrics:\n",
    "\n",
    "- utilization (are the resources on average busy doing things or idle ?)\n",
    "- saturation (are the resources too busy doing things hence unable to handle any request ?)\n",
    "- error (are the service functioning or failing ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b49e50",
   "metadata": {},
   "source": [
    "In the case of this network max-flow service, I would let the system generate low-priority warnings for the following events:\n",
    "\n",
    "- temporary cluster saturation, CPU spikes (should trigger auto-scaling)\n",
    "- temporary errors (caused by provider instability etc.)\n",
    "- low cluster utilization (again, should trigger automated down-scaling)\n",
    "\n",
    "On the other hand, these events should trigger high-priority warnings and alarms:\n",
    "\n",
    "- long cluster satuaration (> 30 seconds), should look for malconfiguration and security incidents\n",
    "- long stream of errors (> 100 continuous error entries), should identify the cause and if it is source-related, rollback immdiately\n",
    "- infrastructure provider warnings (scheduled maintenance or downtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711643c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99f04d",
   "metadata": {},
   "source": [
    "### Unit Tests\n",
    "\n",
    "We should maintain over 90% unit test coverage. It should be a hard metric set in the build pipeline: no unit test coverage no deploy.\n",
    "\n",
    "### Performance Tests\n",
    "\n",
    "Also run performance-testing on every merge. Should upload the time records to a vault (like AWS parameter store) and make sure performance does not degrade over releases.\n",
    "\n",
    "For developers, performance-test programs should be easy to run and easy to understand just like unit tests.\n",
    "\n",
    "### Functional Tests\n",
    "\n",
    "Test the API end points via functional testing. \n",
    "\n",
    "There is space for creativity: \n",
    "- typically one could write func-test in cucumber or python/ruby with custom executor\n",
    "- but there also exists more automated and integrated approaches, such as using the function docstring to perform certain testing (doctest)\n",
    "\n",
    "I'm personally not a big fun of manual functional-testing, but under tight resource constraint this could serve as a temporary strategy while the team progressively automates the functional testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380429c",
   "metadata": {},
   "source": [
    "## System Hardening Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2d0c8",
   "metadata": {},
   "source": [
    "These are not addressed in the requirements but I thought I would share these ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e3f11",
   "metadata": {},
   "source": [
    "### Lowering Risks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564dc6c",
   "metadata": {},
   "source": [
    "The current design assumes any anonymous user (any \"engineer\") can interact with the system which is highly risky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e76a76",
   "metadata": {},
   "source": [
    "#### The short term hardening options\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62cd9c",
   "metadata": {},
   "source": [
    "- limited amount of API actions per IP per minute (configurable via API Gateway)\n",
    "- completely disable SSH ports on the compute instances and use managed console access (such as AWS System Manager Session) if need to. This would keep track of every login in the CloudTrail and trigger security alarm if escalation happens.\n",
    "- set max instance life time in the auto-scaling group to a relatively short duration (6 hours to 1 day). Instances that live longer than this limit are automatically torn down and replaced by newly spawned instances. This prevents long attack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f4e4f",
   "metadata": {},
   "source": [
    "#### The longer term hardening options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f9fd7",
   "metadata": {},
   "source": [
    "- disable anonymous access\n",
    "- require login from, for example, google account; use Identity Federation to manage the user accounts and control their access "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca0136",
   "metadata": {},
   "source": [
    "### Bucket Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff22fed",
   "metadata": {},
   "source": [
    "The S3 bucket policy should follow the least-privilage principle and disable override.\n",
    "\n",
    "- encrypt at rest\n",
    "- block public access\n",
    "- enable versioning\n",
    "- disable `Delete*, Update*` blobs; only allow `Get*, Describe*, Put*, ` blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a37f2b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "one-off",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
